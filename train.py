import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'

import torch
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from tqdm import tqdm
from dataclasses import asdict

from datas.dataset import StableDataset, collate_fn
from datas.sampler import DistributedBucketSampler
from text import symbols
from config import MelConfig, ModelConfig, TrainConfig
from models.model import StableTTS

from utils.scheduler import get_cosine_schedule_with_warmup
from utils.load import continue_training

torch.backends.cudnn.benchmark = True
    
def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12345'
    dist.init_process_group("gloo" if os.name == "nt" else "nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()
    
def _init_config(model_config: ModelConfig, mel_config: MelConfig, train_config: TrainConfig):
    
    if not os.path.exists(train_config.model_save_path):
        print(f'Creating {train_config.model_save_path}')
        os.makedirs(train_config.model_save_path, exist_ok=True)

def train(rank: int, world_size: int) -> None:
    """
    Training function for distributed training.
    
    Args:
        rank: Process rank
        world_size: Total number of processes
    """
    try:
        setup(rank, world_size)
        torch.cuda.set_device(rank)

        model_config = ModelConfig()
        mel_config = MelConfig()
        train_config = TrainConfig()
        
        _init_config(model_config, mel_config, train_config)
        
        model = StableTTS(len(symbols), mel_config.n_mels, **asdict(model_config)).to(rank)
        
        model = DDP(model, device_ids=[rank])

        try:
            train_dataset = StableDataset(train_config.train_dataset_path, mel_config.hop_length)
        except Exception as e:
            if rank == 0:
                print(f"Error loading dataset: {e}")
            cleanup()
            return
        
        train_sampler = DistributedBucketSampler(
            train_dataset, train_config.batch_size, 
            [32,300,400,500,600,700,800,900,1000], 
            num_replicas=world_size, rank=rank
        )
        train_dataloader = DataLoader(
            train_dataset, batch_sampler=train_sampler, 
            num_workers=4, pin_memory=True, 
            collate_fn=collate_fn, persistent_workers=True
        )
        
        if rank == 0:
            writer = SummaryWriter(train_config.log_dir)

        optimizer = optim.AdamW(model.parameters(), lr=train_config.learning_rate)
        scheduler = get_cosine_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=int(train_config.warmup_steps), 
            num_training_steps=train_config.num_epochs * len(train_dataloader)
        )
        
        # load latest checkpoints if possible
        current_epoch = continue_training(train_config.model_save_path, model, optimizer)

        model.train()
        for epoch in range(current_epoch, train_config.num_epochs):
            train_dataloader.batch_sampler.set_epoch(epoch)
            if rank == 0:
                dataloader = tqdm(train_dataloader, desc=f"Epoch {epoch}")
            else:
                dataloader = train_dataloader
            
            for batch_idx, datas in enumerate(dataloader):
                try:
                    datas = [data.to(rank, non_blocking=True) for data in datas]
                    x, x_lengths, y, y_lengths, z, z_lengths = datas
                    optimizer.zero_grad()
                    dur_loss, diff_loss, prior_loss, _ = model(x, x_lengths, y, y_lengths, z, z_lengths)
                    loss = dur_loss + diff_loss + prior_loss
                    loss.backward()
                    optimizer.step()
                    scheduler.step()
                    
                    if rank == 0 and batch_idx % train_config.log_interval == 0:
                        steps = epoch * len(dataloader) + batch_idx
                        writer.add_scalar("training/diff_loss", diff_loss.item(), steps)
                        writer.add_scalar("training/dur_loss", dur_loss.item(), steps)
                        writer.add_scalar("training/prior_loss", prior_loss.item(), steps)
                        writer.add_scalar("training/total_loss", loss.item(), steps)
                        writer.add_scalar("learning_rate/learning_rate", scheduler.get_last_lr()[0], steps)
                except Exception as e:
                    if rank == 0:
                        print(f"Error in training step (epoch {epoch}, batch {batch_idx}): {e}")
                    continue
                
            if rank == 0 and epoch % train_config.save_interval == 0:
                try:
                    checkpoint_path = os.path.join(train_config.model_save_path, f'checkpoint_{epoch}.pt')
                    optimizer_path = os.path.join(train_config.model_save_path, f'optimizer_{epoch}.pt')
                    torch.save(model.module.state_dict(), checkpoint_path)
                    torch.save(optimizer.state_dict(), optimizer_path)
                    print(f"Saved checkpoint at epoch {epoch}")
                except Exception as e:
                    print(f"Error saving checkpoint at epoch {epoch}: {e}")
            
            if rank == 0:
                print(f"Epoch {epoch}, Loss: {loss.item():.4f} (dur: {dur_loss.item():.4f}, diff: {diff_loss.item():.4f}, prior: {prior_loss.item():.4f})")

    except Exception as e:
        if rank == 0:
            print(f"Training error: {e}")
        raise
    finally:
        cleanup()
    
torch.set_num_threads(1)
torch.set_num_interop_threads(1)

if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size)